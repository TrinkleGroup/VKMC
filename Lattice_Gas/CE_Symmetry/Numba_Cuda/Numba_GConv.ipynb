{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r2y1eaB4TPuJ"
   },
   "source": [
    "Github path : Lattice_Gas/CE_Symmetry/Numba_Cuda/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xSaJJD1KHq4M"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from numba import cuda, jit, float32, float64, int64, uint8, int16\n",
    "# import cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fw5DCJjSIdYS",
    "outputId": "bcc29782-7fa0-4f71-ee85-3d5293c719bf"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../Symm_Network/SymNetTrials/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5xW6pzhYIgho"
   },
   "outputs": [],
   "source": [
    "FP = \"../../Symm_Network/SymNetTrials/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xScrwF0oI-if"
   },
   "outputs": [],
   "source": [
    "NsitesMax = 1024\n",
    "NChMax = 100\n",
    "NngbMax = 13 # maximum for close packed structs +1 for the site itself     \n",
    "NChSitesMax = NChMax*NngbMax                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SngK_IiXLoZU"
   },
   "outputs": [],
   "source": [
    "# write device function for softplus\n",
    "@cuda.jit(device=True)\n",
    "def softPlus(x, beta, threshold):\n",
    "    if x < -threshold:\n",
    "        return 0.0\n",
    "    elif x > threshold:\n",
    "        return x\n",
    "    else:\n",
    "        return math.log(1.0 + math.exp(beta*x))/beta\n",
    "\n",
    "@cuda.jit(device=True)\n",
    "def gradSoftPlus(x, beta, threshold):\n",
    "    if x < -threshold:\n",
    "        return 0.0\n",
    "    elif x > threshold:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return 1./(1.0 + math.exp(-beta*x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Rc9v2r5pdU8P"
   },
   "outputs": [],
   "source": [
    "# @cuda.jit\n",
    "# def Gcorr(InputImage, Psi, OutImage, OutF, SiteNeighbors, GnnPerms, N_array,\n",
    "#           sp_beta, sp_threshold):\n",
    "@cuda.jit\n",
    "def Gcorr(InputImage, Psi, OutImage, OutF, SiteNeighbors, GnnPerms, N_array,\n",
    "          sp_beta, sp_threshold):\n",
    "    # first get locations\n",
    "    ty = cuda.threadIdx.y\n",
    "    bx, by, bz = cuda.blockIdx.x, cuda.blockIdx.y, cuda.blockIdx.z\n",
    "\n",
    "    bSizeX, bSizeY, bSizeZ = cuda.blockDim.x, cuda.blockDim.y, cuda.blockDim.z\n",
    "\n",
    "    Nsites = N_array[0] \n",
    "    NInCh = N_array[1]\n",
    "    NOutCh = N_array[2]\n",
    "    N_ngb = N_array[3]\n",
    "    Ng = N_array[4]\n",
    "\n",
    "    # Get the necessary output indices\n",
    "    batchInd = bx # which sample the thread is working with\n",
    "    outCh = (by*bSizeY + ty)//Nsites  # thread's output channel\n",
    "    siteInd = (by*bSizeY + ty)%Nsites # which site the conv is over\n",
    "    gInd = bz  # which group operation the thread is handling\n",
    "\n",
    "    # Get the neighborhood of the current site\n",
    "    # in the thread's local memory\n",
    "    NgbIndices = cuda.local.array(shape=(NngbMax,), dtype=int64)\n",
    "    for ngb in range(N_ngb):\n",
    "        NgbIndices[ngb] = SiteNeighbors[siteInd, ngb]\n",
    "\n",
    "    # create the shared arrays to store filters and input elements\n",
    "    InChannel = cuda.shared.array(shape=(NsitesMax,), dtype=float64)\n",
    "    Filter = cuda.shared.array(shape=(NChSitesMax,), dtype=float64)\n",
    "\n",
    "    # Store the Group rotations of nns into shared memory\n",
    "    gnnRotShared = cuda.shared.array(shape=(NngbMax,), dtype=uint8)\n",
    "    if ty < N_ngb:\n",
    "        gnnRotShared[ty] = GnnPerms[gInd, ty]\n",
    "\n",
    "    linSum = 0.\n",
    "    for inCh in range(NInCh):\n",
    "        # First read the input channel into shared memory\n",
    "        for sweep in range(Nsites//bSizeY + 1):\n",
    "            threadSiteInd = sweep*bSizeY + ty\n",
    "            if threadSiteInd < Nsites:\n",
    "                InChannel[threadSiteInd] = InputImage[batchInd, inCh, threadSiteInd]\n",
    "        \n",
    "        # Then read the filter for this input channel\n",
    "        # Apply Group permutation to it as well\n",
    "        for sweep in range((NOutCh*N_ngb)//bSizeY + 1):\n",
    "            threadElemInd = sweep*bSizeY + ty\n",
    "            if threadElemInd < NOutCh*N_ngb:\n",
    "                Filter[threadElemInd] = Psi[inCh, threadElemInd]\n",
    "        \n",
    "        # synchronize the block\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # Reading phase is done - now convolve\n",
    "        for ngb in range(N_ngb):\n",
    "            ngbSite = NgbIndices[ngb]\n",
    "            linSum += Filter[outCh*N_ngb + gnnRotShared[ngb]] * InChannel[ngbSite]\n",
    "        \n",
    "        # synchronize the block\n",
    "        cuda.syncthreads()\n",
    "        \n",
    "    OutF[batchInd, outCh, gInd, siteInd] = linSum\n",
    "    \n",
    "    nonLin = softPlus(linSum, sp_beta, sp_threshold)/Ng\n",
    "    # atomically sum out the group channel\n",
    "    cuda.atomic.add(OutImage, (batchInd, outCh, siteInd), nonLin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gIpgfeMP_Zzm"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BSQDvKTqIOBM",
    "outputId": "739aa3aa-46b6-4732-f211-f38cb3b94a93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 512 48\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "NNSites = np.load(FP + \"NNsites_sitewise.npy\").T\n",
    "GNNperms = np.load(FP + \"GroupNNpermutations.npy\")\n",
    "RtoSiteInd = np.load(FP + \"RtoSiteInd.npy\")\n",
    "SiteIndtoR = np.load(FP + \"SiteIndtoR.npy\")\n",
    "(Nsites, N_ngb) = NNSites.shape\n",
    "Ng = GNNperms.shape[0]\n",
    "print(N_ngb, Nsites, Ng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkg_6PjwKwfp"
   },
   "outputs": [],
   "source": [
    "# # load the pickle files\n",
    "# import pickle\n",
    "# with open(FP + \"supercellBCC.pkl\", \"rb\") as fl:\n",
    "#     superBCC = pickle.load(fl)\n",
    "\n",
    "# with open(FP + \"GroupOpsIndices.pkl\", \"rb\") as fl:\n",
    "#     GIndices = pickle.load(fl)\n",
    "\n",
    "# with open(FP + \"jnetBCC.pkl\") as fl:\n",
    "#     jNetBCC = pickle.load(fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kBYKzC30LNim"
   },
   "outputs": [],
   "source": [
    "# Create a random input and output image map\n",
    "Nsites = 512\n",
    "Nbatch = 512\n",
    "NchIn = 16\n",
    "NchOut = 16\n",
    "\n",
    "InImage = np.random.rand(Nbatch, NchIn, Nsites)\n",
    "OutImage = np.zeros((Nbatch, NchOut, Nsites))\n",
    "OutF = np.zeros((Nbatch, NchOut, Ng, Nsites))\n",
    "\n",
    "# Now set up a random filter\n",
    "Psi = np.random.rand(NchIn, NchOut*N_ngb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "hr6YrpqWPVWT"
   },
   "outputs": [],
   "source": [
    "ty = 512\n",
    "NbatchRun = Nbatch\n",
    "bX = NbatchRun\n",
    "bY = int(np.ceil((NchOut*Nsites)/ty))\n",
    "bZ = Ng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nsJcgY8kpmJy",
    "outputId": "8cd815b4-b85c-4a53-b4e5-b5b8de19c5e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.5 ms, sys: 497 ms, total: 508 ms\n",
      "Wall time: 603 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "d_input = cuda.to_device(InImage)\n",
    "d_NNSites = cuda.to_device(NNSites)\n",
    "d_GNNperms = cuda.to_device(GNNperms)\n",
    "d_Psi = cuda.to_device(Psi)\n",
    "\n",
    "N_array = np.array([Nsites, NchIn, NchOut, N_ngb, Ng],dtype=int)\n",
    "d_Narray = cuda.to_device(N_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FvEQYhzsJYm_",
    "outputId": "f2f1a53a-2608-44bd-e1f6-9daf0297227f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.7 ms, sys: 29.2 ms, total: 80.9 ms\n",
      "Wall time: 89.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "d_output = cuda.to_device(OutImage)\n",
    "d_outF = cuda.to_device(OutF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "a-GzRHMmJo4x"
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "imbxB8JbpTta",
    "outputId": "b53274f1-1550-4125-93dd-cedc3819c9aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time : 0.294343090057373\n"
     ]
    }
   ],
   "source": [
    "Nrun = 5\n",
    "start = time.time()\n",
    "for i in range(Nrun):\n",
    "    Gcorr[(bX, bY, bZ), (1, ty, 1)](d_input, d_Psi, d_output, d_outF, d_NNSites,\n",
    "                                    d_GNNperms, d_Narray, 1.0, 20.0)\n",
    "    cuda.synchronize()\n",
    "print(\"time : {}\".format((time.time() - start)/Nrun))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "QKVR1HVL0jUz"
   },
   "outputs": [],
   "source": [
    "OutImage = np.zeros((Nbatch, NchOut, Nsites))\n",
    "OutF = np.zeros((Nbatch, NchOut, Ng, Nsites))\n",
    "\n",
    "d_output = cuda.to_device(OutImage)\n",
    "d_outF = cuda.to_device(OutF)\n",
    "\n",
    "cuda.synchronize()\n",
    "Gcorr[(bX, bY, bZ), (1, ty, 1)](d_input, d_Psi, d_output, d_outF, d_NNSites,\n",
    "                                    d_GNNperms, d_Narray, 1.0, 20.0)\n",
    "cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "g-7F58xiCwED"
   },
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def softPlusCPU(x, beta, threshold):\n",
    "    if x < -threshold:\n",
    "        return 0.0\n",
    "    elif x > threshold:\n",
    "        return x\n",
    "    else:\n",
    "        return math.log(1.0 + math.exp(beta*x))/beta\n",
    "\n",
    "@jit(nopython=True)\n",
    "def CorrSerial(InImage, Psi, NchOut, NchIn, Nsites, Ng, NNSites, GNNperms):\n",
    "    outSamp = np.zeros((InImage.shape[0], NchOut, Nsites))\n",
    "    outSampF = np.zeros((InImage.shape[0], NchOut, Ng, Nsites))\n",
    "    for sampInd in range(InImage.shape[0]):\n",
    "        for outCh in range(NchOut):\n",
    "            for siteInd in range(Nsites):\n",
    "                # Now go through the input channels\n",
    "                gsum = 0.\n",
    "                for gInd in range(Ng):\n",
    "                    linSum = 0.\n",
    "                    for inCh in range(NchIn):\n",
    "                        for ngb in range(N_ngb):\n",
    "                            filt = Psi[inCh, outCh*N_ngb + GNNperms[gInd, ngb]]\n",
    "                            linSum += filt * InImage[sampInd, inCh, NNSites[siteInd, ngb]]\n",
    "                    \n",
    "                    outSampF[sampInd, outCh, gInd, siteInd] = linSum\n",
    "                    \n",
    "                    gsum += softPlusCPU(linSum, 1, 20)/Ng\n",
    "\n",
    "                outSamp[sampInd, outCh, siteInd] = gsum\n",
    "    return outSamp, outSampF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55.4 s, sys: 180 ms, total: 55.6 s\n",
      "Wall time: 55.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "out,outF=CorrSerial(InImage, Psi, NchOut, NchIn, Nsites, Ng, NNSites, GNNperms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "HostOut = d_output.copy_to_host()\n",
    "HostOutF = d_outF.copy_to_host()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert np.allclose(HostF[sampInd], outSampF)\n",
    "assert np.allclose(HostOut, out)\n",
    "assert np.allclose(HostOutF, outF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ox-yA2FZqnFE"
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def GConvBack(dL_dOutImg, InImg, Psi, dL_dInImg, NbgSites, Narray,\n",
    "              sp_beta, sp_threshold):\n",
    "\n",
    "    ty = cuda.threadIdx.y\n",
    "    bSizeX, bSizeY, bSizeZ = cuda.blockIdx.x, cuda.blockIdx.y, cuda.blockIdx.z\n",
    "    \n",
    "    Nsites = N_array[0] \n",
    "    NInCh = N_array[1]\n",
    "    NOutCh = N_array[2]\n",
    "    N_ngb = N_array[3]\n",
    "    Ng = N_array[4]\n",
    "\n",
    "    # Get the necessary output indices\n",
    "    batchInd = bx # which sample the thread is working with\n",
    "    outCh = (by*bSizeY + ty)//Nsites  # thread's output channel\n",
    "    siteInd = (by*bSizeY + ty)%Nsites # which site the conv is over\n",
    "    gInd = bz  # which group operation the thread is handling\n",
    "\n",
    "    # First compute the linear correlation term\n",
    "    \n",
    "    # Get the neighborhood of the current site\n",
    "    # in the thread's local memory\n",
    "    NgbIndices = cuda.local.array(shape=(NngbMax,), dtype=int64)\n",
    "    for ngb in range(N_ngb):\n",
    "        NgbIndices[ngb] = SiteNeighbors[siteInd, ngb]\n",
    "\n",
    "    # create the shared arrays to store filters and input elements\n",
    "    InChannel = cuda.shared.array(shape=(NsitesMax,), dtype=float64) # max 8kB\n",
    "    Filter = cuda.shared.array(shape=(NChSitesMax,), dtype=float64) # max 8kB+\n",
    "    # create a shared array in which threads write their contribution to\n",
    "    # the gradient\n",
    "    dL_dIn_shared = cuda.shared.array(shape=(NsitesMax,), dtype=float64) # max 8KB\n",
    "\n",
    "    # Store the Group rotations of nns into shared memory\n",
    "    gnnRotShared = cuda.shared.array(shape=(NngbMax,), dtype=uint8) # max 0.1 kB\n",
    "    if ty < N_ngb:\n",
    "        gnnRotShared[ty] = GnnPerms[gInd, ty]\n",
    "\n",
    "    linSum = 0.\n",
    "    for inCh in range(NInCh):\n",
    "        # First read the input channel into shared memory\n",
    "        for sweep in range(Nsites//bSizeY + 1):\n",
    "            threadSiteInd = sweep*bSizeY + ty\n",
    "            if threadSiteInd < Nsites:\n",
    "                InChannel[threadSiteInd] = InImg[batchInd, inCh, threadSiteInd]\n",
    "        \n",
    "        # Then read the filter for this input channel\n",
    "        for sweep in range((NOutCh*N_ngb)//bSizeY + 1):\n",
    "            threadElemInd = sweep*bSizeY + ty\n",
    "            if threadElemInd < NOutCh*N_ngb:\n",
    "                Filter[threadElemInd] = Psi[inCh, threadElemInd]\n",
    "        \n",
    "        # synchronize the block\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # Reading phase is done - now convolve\n",
    "        for ngb in range(N_ngb):\n",
    "            ngbSite = NgbIndices[ngb]\n",
    "            linSum += Filter[outCh*N_ngb + gnnRotShared[ngb]] * InChannel[ngbSite]\n",
    "        \n",
    "        # synchronize the block\n",
    "        cuda.syncthreads()\n",
    "        \n",
    "    # once the linear sum for this layer is found, we need to\n",
    "    # add its contribution to the relevant components of the gradient\n",
    "    nonLinGrad = gradSoftPlus(linSum, sp_beta, sp_threshold)\n",
    "\n",
    "    gradOut = dL_dOutImg[batchInd, outCh, siteInd]\n",
    "\n",
    "    for inCh in range(NInCh): # iterate over input channels\n",
    "        # zero out the block sum for this input channel\n",
    "        for sweep in range(Nsites//bSizeY + 1):\n",
    "            threadSiteInd = sweep*bSizeY + ty\n",
    "            if threadSiteInd < Nsites:\n",
    "                dL_dIn_shared[threadSiteInd] = 0.\n",
    "\n",
    "        # Read in the filter elements\n",
    "        for sweep in range((NOutCh*N_ngb)//bSizeY + 1):\n",
    "            threadElemInd = sweep*bSizeY + ty\n",
    "            if threadElemInd < NOutCh*N_ngb:\n",
    "                Filter[threadElemInd] = Psi[inCh, threadElemInd]\n",
    "        \n",
    "        # synchronize to ensure all reads are complete\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        for ngb in range(N_ngb): # iterate over nearest neighbors to write to\n",
    "            ngbSite = NgbIndices[ngb]\n",
    "            # compute the current thread's contribution to this current nearest\n",
    "            # neighbor\n",
    "            val = nonLinGrad * gradOut * Filter[outCh*N_ngb + gnnRotShared[ngb]]\n",
    "            cuda.atomicadd(dL_dIn_shared[ngbSite], val)\n",
    "        \n",
    "        # wait for atomic adds to get completed\n",
    "        cuda.synchthreads()\n",
    "\n",
    "        # Once the neighbor contributions are accumulated, now sum out the group\n",
    "        # and output channels\n",
    "        # threads of the same batch, different out channels and different\n",
    "        # group channels must add to the same site.\n",
    "        cuda.atomicadd(dL_dInImg[batchInd, inCh, siteInd], dL_dIn_shared[siteInd])\n",
    "\n",
    "        # sync the threads again before zeroing out the shared array\n",
    "        cuda.synchthreads()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Numba_GConv.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
